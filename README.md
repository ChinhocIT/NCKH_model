# GDA System - Global Description Acquisition

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**Há»‡ thá»‘ng Visual Question Answering (VQA) thá»i gian thá»±c vá»›i kháº£ nÄƒng phÃ¢n Ä‘oáº¡n váº­t thá»ƒ vÃ  tÆ°Æ¡ng tÃ¡c giá»ng nÃ³i.**

## âœ¨ TÃ­nh nÄƒng

- ğŸ¯ **PhÃ¢n Ä‘oáº¡n váº­t thá»ƒ chÃ­nh xÃ¡c** vá»›i SAM 2 (Segment Anything Model)
- ğŸ§  **Vision-Language understanding** vá»›i Qwen2-VL
- ğŸ—£ï¸ **TÆ°Æ¡ng tÃ¡c giá»ng nÃ³i** (Vietnamese + English)
- âš¡ **Real-time inference** trÃªn webcam
- ğŸ¨ **Semantic segmentation** vá»›i SETR decoder (COCO-Stuff 171 classes)
- ğŸ”§ **Modular architecture** dá»… má»Ÿ rá»™ng

## ğŸ—ï¸ Kiáº¿n trÃºc

```
Input Image â†’ ViT Encoder â†’ [Seg Decoder + Adaptor] â†’ Vision Tokens â†’ LLM â†’ Answer
                  â†“
              SAM 2 Mask
```

### Components chÃ­nh:

1. **Shared ViT Encoder**: TrÃ­ch xuáº¥t visual features tá»« Qwen2-VL
2. **SETR Segmentation Decoder**: Dá»± Ä‘oÃ¡n class cho tá»«ng vÃ¹ng
3. **Vision-Language Adaptor**: Chuyá»ƒn Ä‘á»•i visual features â†’ language embeddings
4. **SAM 2 Segmenter**: PhÃ¢n Ä‘oáº¡n váº­t thá»ƒ tá»« user click
5. **LLM Generator**: Qwen2-VL language model sinh cÃ¢u tráº£ lá»i

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **GPU**: NVIDIA GPU vá»›i CUDA 11.8+ (khuyáº¿n nghá»‹ â‰¥8GB VRAM)
- **RAM**: 16GB+
- **OS**: Windows/Linux/macOS

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository

```bash
git clone https://github.com/yourusername/gda-system.git
cd gda-system
```

### 2. Táº¡o mÃ´i trÆ°á»ng áº£o

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate     # Windows
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 4. Download models

```bash
python scripts/download_models.py
```

### 5. Cáº¥u hÃ¬nh

Táº¡o file `.env` tá»« template:

```bash
cp .env.example .env
```

Chá»‰nh sá»­a `.env`:

```env
# Model paths
QWEN_MODEL_NAME=Qwen/Qwen2-VL-2B-Instruct
SAM_MODEL_NAME=facebook/sam-vit-huge
SEG_CHECKPOINT_PATH=checkpoints/seg_decoder_best.pth
ADAPTOR_CHECKPOINT_PATH=checkpoints/adaptor_best.pth

# Device
DEVICE=cuda
DEBUG=False

# Voice
ENABLE_STT=True
ENABLE_TTS=True
```

## ğŸ’¡ Sá»­ dá»¥ng

### Basic Usage

```bash
python app.py
```

### Advanced Options

```bash
# Chá»‰ Ä‘á»‹nh checkpoint
python app.py --seg-checkpoint path/to/seg.pth --adaptor-checkpoint path/to/adaptor.pth

# Enable debug mode
python app.py --debug

# Sá»­ dá»¥ng CPU
python app.py --device cpu
```

### Keyboard Controls

| PhÃ­m | Chá»©c nÄƒng |
|------|-----------|
| `Space` | KÃ­ch hoáº¡t cháº¿ Ä‘á»™ chá»n vÃ¹ng |
| `C` (giá»¯) + Voice | Há»i cÃ¢u há»i báº±ng giá»ng nÃ³i |
| `Enter` | MÃ´ táº£ tá»± Ä‘á»™ng vÃ¹ng Ä‘Ã£ chá»n |
| `S` | LÆ°u áº£nh hiá»‡n táº¡i |
| `D` | Toggle debug mode |
| `Q` | ThoÃ¡t |

### Python API

```python
from src.core.gda import GlobalDescriptionAcquisition
import cv2

# Initialize
gda = GlobalDescriptionAcquisition(device="cuda")

# Load image
image = cv2.imread("image.jpg")
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Segment object (click point)
mask = gda.sam_segmenter.segment_from_point(image_rgb, point=(320, 240))

# Ask question
result = gda.process_region(image_rgb, mask, user_query="ÄÃ¢y lÃ  gÃ¬?")

print(result['description'])
# Output: "ÄÃ¢y lÃ  má»™t chiáº¿c laptop mÃ u xÃ¡m, cÃ³ bÃ n phÃ­m Ä‘en vÃ  mÃ n hÃ¬nh Ä‘ang báº­t."
```

## ğŸ“š Documentation

- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api.md)
- [Training Guide](docs/training.md)
- [Deployment Guide](docs/deployment.md)

## ğŸ§ª Testing

```bash
# Cháº¡y táº¥t cáº£ tests
pytest tests/

# Test vá»›i coverage
pytest --cov=src tests/

# Test specific module
pytest tests/test_models.py
```

## ğŸ“ Training

### Train Segmentation Decoder

```bash
python scripts/train_decoder.py \
  --dataset coco_stuff \
  --epochs 50 \
  --batch-size 8 \
  --lr 1e-4
```

### Train Vision-Language Adaptor

```bash
python scripts/train_adaptor.py \
  --dataset vqa_v2 \
  --epochs 20 \
  --batch-size 4
```

## ğŸ“Š Performance

| Model | GPU | FPS | Accuracy |
|-------|-----|-----|----------|
| Full System | RTX 3090 | ~2-3 | 85%+ |
| Seg Decoder only | RTX 3090 | ~10 | 78% mIoU |
| SAM 2 only | RTX 3090 | ~8 | 92% IoU |

## ğŸ¤ Contributing

ChÃºng tÃ´i hoan nghÃªnh má»i Ä‘Ã³ng gÃ³p! Vui lÃ²ng Ä‘á»c [CONTRIBUTING.md](CONTRIBUTING.md) Ä‘á»ƒ biáº¿t chi tiáº¿t.

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Setup pre-commit hooks
pre-commit install

# Run linting
black src/
flake8 src/
mypy src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- **Qwen2-VL**: Alibaba Cloud
- **SAM 2**: Meta AI
- **SETR**: Fudan University
- **COCO-Stuff**: Stanford University

## ğŸ“ Contact

- **Author**: Your Name
- **Email**: your.email@example.com
- **GitHub**: [@yourusername](https://github.com/yourusername)

## ğŸ“ˆ Roadmap

- [ ] Support multiple languages
- [ ] Add batch processing mode
- [ ] Integrate with mobile app
- [ ] Cloud deployment guide
- [ ] Pre-trained checkpoints release
- [ ] Docker container
- [ ] Web demo

## â­ Citation

```bibtex
@software{gda_system,
  author = {Your Name},
  title = {GDA System: Global Description Acquisition},
  year = {2025},
  url = {https://github.com/yourusername/gda-system}
}
```